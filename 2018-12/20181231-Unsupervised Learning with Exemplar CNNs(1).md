# Unsupervised Learning with Exemplar CNNs

As [mentioned in the note before](https://github.com/WenjayDu/ML_Study_Notes/blob/master/2018-12/20181227-U_Net%281%29.md), today's paper prove the "value of data augmentation for learning invariance" in the field of unsupervised feature learning.

Recent years, supervised CNNs trained with BP perform better than before on CV tasks. And authors refer to a method that a NN already trained can be adapted to new tasks, on this basis, retraining can be carried on smaller data sets. This method is like the one mentioned in yesterday's note that uses a pre-trained NN to learn from a small data set. However, they say this method has drawbacks that one is pre-trained supervised NN still needs big labeled datasets, and the other one is pre-trained NNs' performance will become worse if new tasks become more different from original ones.

After discussions above, authors present a method that train a CNN with unlabeled data which is generated by random data augmentation techniques(so this is an unsupervised learning), but different from previous techniques, this method has a concept called "seed image" that means a seed image is used to build a whole class which is called "surrogate class". The trained NN is called Exemplar-CNN. Authors state their NN can learn invariant features. ⬅️ This feature is mentioned in the note referred to at the beginning. I will learn more about it tomorrow.


## References
1. Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative unsupervised feature learning with convolutional neural networks. In: NIPS (2014) .