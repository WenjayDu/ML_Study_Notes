# About ResNet

ResNet was proposed in 2015 by Kaiming He team and won 1st place in ImageNet Challenge. As we all know that, conventional deep NNs have difficulities as their depth increasing, while reaching a certain degree, deeper NNs' performance even become poorer than shallower ones. Even when gradient dispersion and gradient explosion are got over by methods like BN etc., this situation cannot be alleviated. In many papers, people call this phenomenon "Degradation". Not like overfitting, degradation makes NN have high error rate even in trainning data sets. However, in ResNet, degradation seems be deterred.

In He's paper[1], they propose a concept called "Shortcut Connection" which is similar with the concept of "Highway" in Highway Network, and He states in the paper that shortcut connection performs better than "Highway" implemented by gating mechanism, for shortcut has no parameters but gating mechanism does. The shortcut connection implemented by identity mapping is quite easy to be understood. In the ResNet building block, the shortcut conneciton enables input to be directly added with result processed by Conv and BN layer. That is the key. Of course, in practise, the whole network architecture will be more complex. But this thought of network design and amazing performance indeed confuse many people.

In paper[2], author illustrates that the reason why ResNet can behave well is that its shortcut connection makes outgoing weights identifiable which means it can maintain deep NNs' expression ability and complexity. In contrast, degradation makes deep NN become simple inside, although they look very complex outside. (emmmmm, this is my opinion from the paper , maybe not correct)

To answer the question "If resnets are the answer, then what is the question?", paper[3] proposes a very interesting concept that is "Shattered Gradients". Author thinks, like value information which has correlation between each other, gradients do too.
And degradation is caused by gradients in back propagation having becoming white noise so that weight updating becomes meaningless, hence, network cannot learn much from data. But in ResNet, Gradients' shattering rate can be suppressed at lower levels that feedfoward net and this is why ResNet can be used to train very deep network.

## References
1. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR. (2016)
2. Emin Orhan, Xaq Pitkow: Skip Connections Eliminate Singularities. 2016.
3. David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, Brian McWilliams: The Shattered Gradients Problem: If resnets are the answer, then what is the question? 2017.