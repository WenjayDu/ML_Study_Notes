# MorphNet

As authors said that DNNs in the field of CV often have residual connections which may change the topology of the network and produce new kinds of connectivity which did not exist in the network before. This may cause serious complication in the net structure when it has residual connections between tens of layers. Therefore, to preserve the network topology, authors group all neurons that are tied in residual connections via a Group LASSO, ðŸš©which I will learn about latter.

It is noteworthy that when targeting different resources (FLOPs and model size), the FLOP regularizer tends to remove neurons from the lower layers which are closer to the input, in the contrast, the model size regularizer tends to remove neurons form upper layers which are closer to the output. The explanation of this phenomenon is the lower layers near the input are applied to a high-resolution image, and thereby consume a large quantity of FLOPs. Relatively, the upper layers where the number of filter are usually bigger, therefore containing larger weight matrices.

Interestingly, applying MorphNet to ResNet can make MorphNet also learn to shrink the number of layers. This happens when all residual filters in a layer are pruned, in that situation, the output of this layer is a copy of the input, hence, the layer can be removed.


## References
1. Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, Edward Choi: *MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks*. In 2018.