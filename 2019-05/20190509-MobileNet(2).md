# MobileNet

Except the first layer which is a FCL, the MobileNet structure is built on depth-wise separable convolutions.

The MobileNet architecture spend 95% of the computation time into dense 1x1 convolutions, which can be directly implemented with one of the most optimized numerical linear algebra algorithms, general matrix multiply (GEMM) functions, which are often used to implement convolutions. However, different from other common convolutions that require an initial reordering in memory called im2col in order to map it to GEMM, these 1x1 convolutions don't need this step in implementation. In MobileNets, 1x1 convolutions have 75% of params, therefore, latency is low.

Because MobileNets are small networks that have less trouble with overfitting, authors use less regularization and data augmentation techs than training large models.

To make models smaller and faster, authors introduce a simple hyper parameter called width multiplier, which works in the same way with the one works in the shrinking stage in MorphNet, to thinner models and reducing computational cost.


## References
1. Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam: *MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications*. In 2017.