# MorphNet

MorphNet is an algo that can optimize the DNN by iteratively shrinking via a resource weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers.

In one iteration, shrinking (steps 1-2) applies a sparsifying regularizer on neurons to remove unimportant ones, making the network consume less targeted resource, at the cost of performance. But this stage can help find over-parameterized layers and layers which are bottlenecked, which is instructive for the expanding stage (usually step 3). In expansion, authors expand all layer sizes via a width multiplier as much as the constrained resource allows. This can help put greater cost on neurons those make more contribution to the network performance, because for 2 layers both containing 100 neurons, one layer makes more contribution to performance, while the other makes less, after the shrinking stage, the former will own more neurons than the latter, assuming the former has 80 neurons now and the latter has only 50. Assuming the multiplier is 1.5, after the expanding stage, the former will have 120 neurons, 20 more than before shrinking, and the latter will have only 75 neurons, 25 less than before shrinking. 

This is very interesting, through the two stages, shrinking and expanding, against a specific resource (FLOPs or the mobile size, namely the number of params), we can optimize the allocation of resource, thereby improving the performance of the network.


## References
1. Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, Edward Choi: *MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep Networks*. In 2018.