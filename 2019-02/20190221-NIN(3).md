# Network In Network

Experiments show that fully connected layers will overfit to training data if without regularizers. Adding dropout as regularizer before the FCL can help reduce the testing error. Replacing FCL and dropout with GAP makes the testing error the lowest among the three.

Not only for NIN, GAP has the same regularization effect for conventional CNNs.

By visualizing feature maps, authors show that strongest activations appear roughly in the region of the object, especially structured objects. NIN obtain this result via the combination of mlpconv layers and global average pooling. The former enables NIN to have a stronger receptive filed modeling. And the latter enforce the learning of category level feature maps.

## References
1. Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. In 2013. 