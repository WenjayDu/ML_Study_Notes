# FitNet

Compared with knowledge distillation that aims at obtaining small and agile models, FitNets presented in this paper allow NNs trained with not only outputs but also intermediate representations learned by the teacher to become thinner and deeper. Authors explain this as the student's hidden layers are usually smaller than the teacher's, "additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer".

In this paper, authors also refer to 2 way to compress NNs: one is to train a student to mimic the teacher; the other is KD(Knowledge distillation). However, authors claim that all previous works like these did not take advantage of depth, which is a vital point of representation learning. By contrast, FitNets, a kind of thin and deep NN just like its name, is the use of depth to handle the problem of model compression. Deeper means better generalization, thinner means less computation.

## References
1. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio: FitNets: Hints for Thin Deep Nets. In ICLR, 2015.