# Dropout

Authors also modify the penalty term -- they set a max norm constraint on the incoming weight vector for each individual hidden unit to prevent weights from growing too large, rather than traditionally penalizing the whole weight vector. This modification makes it possible to start training with a quite large learn rate which decays during the learning, and the benefit is it allows a more thorough search of weight-space.

Different from the training phase, at test time, all hidden units will be activated with their outgoing weights halved because half(0.5) hidden units are randomly ignored during training.



## References
1. Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-dinov. Improving neural networks by preventing co-adaptation of feature detectors.