# FitNet

The whole training process is stage-wise: the first stage is pre-training the student NN with the output of the hint layer in the teacher NN; the second stage is the knowledge distillation training of the whole NN.

Authors argue that their training method is a form of Curriculum Learning(CL), which was introduced in the paper[2] and I will read and learn about it later. Different from CL methods present previously that are problem-specific, authors' approach is more generic. Because their algorithm uses a param to control the weight of the teacher cross-entropy, initially this param is big enough to allow the student learn more from the teacher on easy examples and then decrease the value of this param in the training process to train the whole student NN.

Experiments and comparisons tell us that hint-based training can help us train deeper NNs and these NNs are able to out-perform those trained with classification targets.


## References
1. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio: FitNets: Hints for Thin Deep Nets. In ICLR, 2015.
2. Bengio, Y.: Curriculum Learning. In 2009. 