# Dropout

Why authors set the dropout probability to 0.5 in this paper? Why not other values? Actually, authors have tried various probabilities and find that almost all of them can improve the performance of the NN, but more extreme ones make the network perform worse. This is the reason why 0.5 is used.

If several different regimes which have different relationships between input and output are required in datasets, performance can be much improved by making dropout probabilities be a learned function of the input.

Furthermore, dropout is much simpler to implement than Bayesian Model Averaging(BMA) which weights each model by its posterior with given training data. If the model is complex, BMA's processing is typically complicated too. In contrast, dropout is easier. A popular alternative to BMA is "bagging" in which different models are trained on different examples randomly selected from the training dataset and all models are given the same weight. Dropout can be seen as a special case of bagging in which each model is trained on the single case and each param is strongly regularized by sharing it with the corresponding param in all the other models. This regularizer is much better than the standard one shrinking params to 0.

## References
1. Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhut-dinov. Improving neural networks by preventing co-adaptation of feature detectors.